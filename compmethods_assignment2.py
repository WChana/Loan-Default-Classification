# -*- coding: utf-8 -*-
"""compmethods_assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13nfe1osyj1FWMs5cPipFpRrXp2M9pX3t
"""

#import dataset from google drive url
!pip install gdown
import gdown
file_id = "1xSamFrBsxKyfwOksHkc9bb1mHpNyoRCQ"
gdown.download(f"https://drive.google.com/uc?id={file_id}", "train.csv", quiet=False)

!pip install graphviz
!pip install scikit-learn imbalanced-learn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import graphviz
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

# read data
data = pd.read_csv("train.csv")
print(data.head())

"""## Exploratory Data Analysis"""

data.info()

null_counts = data.isnull().sum()
print(null_counts)

# Dropping columns due to high number of null values and insignificance towards target
columns_to_drop = [
    'Social_Circle_Default',
    'Score_Source_1',
    'Client_Occupation',
    'Credit_Bureau',
    'Own_House_Age',
    'ID',
    'Score_Source_2',
    'Score_Source_3',
]
data.drop(columns=columns_to_drop, inplace=True)

null_counts = data.isnull().sum()
print(null_counts)

# Drop null values from data
data.dropna(inplace=True)

print(data.size)

#default_1 = data[data['Default'] == 1]
#default_0 = data[data['Default'] == 0]
#subset_default_0 = default_0.sample(n= int(0.1 * len(default_0)), random_state=42)
#balanced_data = pd.concat([subset_default_0, default_1])
#balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)

balanced_data = data.copy()

# Look at the data's distribution of the target variable
balanced_data['Default'].value_counts()

# Ensure numeric variables are numeric (some had some non-numeric characters in them)
numeric_columns = ['Client_Income', 'Credit_Amount', 'Loan_Annuity',
                   'Age_Days', 'Employed_Days', 'Registration_Days',
                   'ID_Days', 'Phone_Change']
for col in numeric_columns:
    balanced_data[col] = pd.to_numeric(balanced_data[col], errors='coerce')

data_updated = balanced_data.copy()

# Encode categorical variables
label_encoder = LabelEncoder()
binary_columns = ['Accompany_Client', 'Client_Income_Type', 'Client_Education',
                   'Client_Marital_Status', 'Client_Gender', 'Loan_Contract_Type',
                   'Client_Housing_Type', 'Population_Region_Relative',
                   'Client_Permanent_Match_Tag', 'Client_Contact_Work_Tag',
                   'Type_Organization']

for col in binary_columns:
  data_updated[col] = data_updated[col].astype(str)
  data_updated[col] = label_encoder.fit_transform(data_updated[col])

data_updated.head()

data_updated.info()

data_updated.shape

# Look at correlation of the data
corr_matrix = data_updated.corr()
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show

# Rank correlation of variables with the target variable
correlation_with_y = corr_matrix['Default'].sort_values(ascending=False)
print(correlation_with_y)

# Selected features based on correlation greater than abs value of 0.02
selected_features = [
    'Cleint_City_Rating',
    'Client_Education',
    'Age_Days',
    'Phone_Change',
    'Client_Gender',
    'ID_Days',
    'Employed_Days',
    'Client_Permanent_Match_Tag',
    'Client_Income_Type',
    'Registration_Days',
    'Application_Process_Hour',
    'Credit_Amount',
    'Type_Organization',
    'Client_Income',
    'Client_Contact_Work_Tag',
    'Loan_Contract_Type',
    'Population_Region_Relative',
    'Car_Owned',
    'Child_Count',
    'Default'
]

data_updated = data_updated[selected_features]

# Scale data and split into train and test split
X = data_updated.drop('Default', axis=1)
y = data_updated['Default']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Drop null values that appeared after scaling
X_train = pd.DataFrame(X_train_scaled)
X_test = pd.DataFrame(X_test_scaled)
y_train = pd.DataFrame(y_train)
y_test - pd.DataFrame(y_test)

X_train, y_train = X_train.align(y_train, join='inner', axis=0)
X_test, y_test = X_test.align(y_test, join='inner', axis=0)

X_train = X_train.dropna()
X_test = X_test.dropna()

y_train = y_train.loc[X_train.index].reset_index(drop=True)
y_test = y_test.loc[X_test.index].reset_index(drop=True)

X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)

y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

#Oversampling data to form a more balanced training set
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

"""## Plain Decision Tree"""

# Create list of hyperparameters to test with grid search and run decision tree
dt_params = {'max_depth': [10, 20, 50],
             'min_samples_split': [2, 5, 10, 15],
             'min_samples_leaf': [1, 2, 5],
             'max_features': [None, 'sqrt', 'log2']}
dt_grid_search = GridSearchCV(DecisionTreeClassifier(), dt_params, cv=5, scoring='accuracy')
dt_grid_search.fit(X_train, y_train)

dt_grid_search.best_params_

clf = dt_grid_search.best_estimator_

feature_cols = [
    'Cleint_City_Rating',
    'Client_Education',
    'Age_Days',
    'Phone_Change',
    'Client_Gender',
    'ID_Days',
    'Employed_Days',
    'Client_Permanent_Match_Tag',
    'Client_Income_Type',
    'Registration_Days',
    'Application_Process_Hour',
    'Credit_Amount',
    'Type_Organization',
    'Client_Income',
    'Client_Contact_Work_Tag',
    'Loan_Contract_Type',
    'Population_Region_Relative',
    'Car_Owned',
    'Child_Count'
]

# Plot tree viz
dot_data = tree.export_graphviz(clf, out_file=None,
                                 feature_names=feature_cols,
                                 class_names=['0', '1'],
                                 filled=True, rounded=True,
                                 special_characters=True)

graph = graphviz.Source(dot_data)
graph.render("decision_tree", format="png", cleanup=True)
graph

dt_y_pred = clf.predict(X_test)
dt_y_probs = dt_grid_search.predict_proba(X_test)[:, 1]
print(classification_report(y_test, dt_y_pred))

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, dt_y_probs)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC - Decision Tree')
plt.legend(loc='lower right')
plt.show()

# Plot precision recall curve
dt_precision, dt_recall, _ = precision_recall_curve(y_test, dt_y_probs)
plt.figure()
plt.plot(dt_recall, dt_precision, label='Decision Tree')
plt.title('Precision-Recall Curve - Decision Tree')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend(loc='upper right')
plt.show()

"""# Random Forest Classifier"""

# List hyperparameters to try for random forest model and run
rf = RandomForestClassifier()
params = {'max_depth': [5, 7, 9],
          'n_estimators': [50, 100, 150],
          'max_features': ['sqrt', 'log2']
          }

rf_grid = GridSearchCV(rf, params, cv=10, scoring='accuracy', return_train_score=False)
rf_grid.fit(X_test, y_test)

print(rf_grid.best_params_)
print(rf_grid.score(X_test, y_test))

clf_rf = rf_grid.best_estimator_

y_pred = clf_rf.predict(X_test)
y_probs = rf_grid.predict_proba(X_test)[:, 1]
print(classification_report(y_test, y_pred))

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC - Random Forest')
plt.legend(loc='lower right')
plt.show()

# Plot precision recall curve
precision, recall, _ = precision_recall_curve(y_test, y_probs)

plt.figure()
plt.plot(recall, precision, color='b', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Plot confusion matrix
y_pred = rf_grid.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Default', 'Default'], yticklabels=['No Default', 'Default'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""# Adaboost"""

# List hyperparameters to test for adaboost
param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.1, 0.5],
}

# Run model via gris search
grid_search = GridSearchCV(estimator=AdaBoostClassifier(random_state=42), param_grid=param_grid,
                           cv=5, scoring='accuracy', n_jobs=-1)

grid_search.fit(X_train, y_train)
grid_search.best_params_

clf = grid_search.best_estimator_
y_pred = clf.predict(X_test)
y_probs = grid_search.predict_proba(X_test)[:, 1]
print(classification_report(y_test, y_pred))

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC - AdaBoost')
plt.legend(loc='lower right')
plt.show()

# Plot precision recall curve
precision, recall, _ = precision_recall_curve(y_test, y_probs)

plt.figure()
plt.plot(recall, precision, color='b', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Default', 'Default'], yticklabels=['No Default', 'Default'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""# XGBoost"""

# List hyperparameters for xgboost
param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 0.3],
    'max_depth': [3, 5, 7]
}

# Run xgboost
xg_grid_search = GridSearchCV(estimator= XGBClassifier(), param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)

xg_grid_search.fit(X_train, y_train)

xg_grid_search.best_params_

clf = xg_grid_search.best_estimator_
y_pred = clf.predict(X_test)
y_probs = grid_search.predict_proba(X_test)[:, 1]
print(classification_report(y_test, y_pred))

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC - XGBoost')
plt.legend(loc='lower right')
plt.show()

# Plot precision recall curve
precision, recall, _ = precision_recall_curve(y_test_clean, y_probs)

plt.figure()
plt.plot(recall, precision, color='b', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()